{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rakshasitaram/tflite_flutter_helper/blob/main/plant_identifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import shutil # For cleaning up directories\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Setup & Paths\n",
        "# ---------------------------\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_google_drive_path = '/content/drive/MyDrive/BitBloom/plant_app/'\n",
        "\n",
        "# Input Directories\n",
        "DATA_DIRS = {\n",
        "    \"train_orig\": os.path.join(base_google_drive_path, 'training'),\n",
        "    \"validation\": os.path.join(base_google_drive_path, 'validation'),\n",
        "    \"test\": os.path.join(base_google_drive_path, 'test')\n",
        "}\n",
        "\n",
        "# Temporary directory for pre-processed masked images (use /tmp for fast I/O)\n",
        "MASKED_TRAIN_DIR = '/tmp/masked_training_data/'\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Config\n",
        "# ---------------------------\n",
        "NUM_CLASSES = 4\n",
        "IMG_HEIGHT, IMG_WIDTH = 224, 224\n",
        "BATCH_SIZE = 32\n",
        "BATCH_SIZE_HALF = BATCH_SIZE // 2\n",
        "EPOCHS_PHASE1 = 10\n",
        "EPOCHS_PHASE2 = 10\n",
        "PATIENCE = 6\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Preprocessing Logic (Runs Once at Start)\n",
        "# ---------------------------\n",
        "\n",
        "def background_removal_single_image(img_path, target_size):\n",
        "    \"\"\"Applies HSV-green masking for a single image loaded from disk.\"\"\"\n",
        "    try:\n",
        "        # Load, resize, and convert to BGR (OpenCV default)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            return None\n",
        "        img = cv2.resize(img, (target_size[1], target_size[0]))\n",
        "\n",
        "        img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "        lower_green = np.array([30, 40, 40])\n",
        "        upper_green = np.array([90, 255, 255])\n",
        "\n",
        "        mask = cv2.inRange(img_hsv, lower_green, upper_green)\n",
        "\n",
        "        # Morphological cleaning\n",
        "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
        "        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "        # Create a 3-channel mask for bitwise_and\n",
        "        mask3 = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
        "        masked_img_bgr = cv2.bitwise_and(img, mask3)\n",
        "\n",
        "        return masked_img_bgr\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {img_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def preprocess_and_save_masked_data(input_dir, output_dir, target_size):\n",
        "    \"\"\"Iterates through training data, masks images, and saves them to a temp directory.\"\"\"\n",
        "    print(\"\\n--- Starting One-Time Preprocessing (Saving to /tmp for speed) ---\")\n",
        "\n",
        "    # 1. Clean and setup target directory\n",
        "    if os.path.exists(output_dir):\n",
        "        shutil.rmtree(output_dir)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    total_images = 0\n",
        "\n",
        "    # Get class directories\n",
        "    class_dirs = [d for d in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, d))]\n",
        "\n",
        "    for class_name in class_dirs:\n",
        "        class_input_path = os.path.join(input_dir, class_name)\n",
        "        class_output_path = os.path.join(output_dir, class_name)\n",
        "        os.makedirs(class_output_path, exist_ok=True)\n",
        "\n",
        "        image_files = [f for f in os.listdir(class_input_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        total_images += len(image_files)\n",
        "\n",
        "        for i, filename in enumerate(image_files):\n",
        "            input_path = os.path.join(class_input_path, filename)\n",
        "            output_path = os.path.join(class_output_path, filename)\n",
        "\n",
        "            masked_img_bgr = background_removal_single_image(input_path, target_size)\n",
        "\n",
        "            if masked_img_bgr is not None:\n",
        "                cv2.imwrite(output_path, masked_img_bgr)\n",
        "\n",
        "            if (i + 1) % 500 == 0:\n",
        "                  print(f\"  Processed {i+1} images in class {class_name}...\")\n",
        "\n",
        "    print(f\"--- Preprocessing Complete. {total_images} images saved to {output_dir} ---\")\n",
        "\n",
        "# Execute the preprocessing step\n",
        "preprocess_and_save_masked_data(DATA_DIRS[\"train_orig\"], MASKED_TRAIN_DIR, (IMG_HEIGHT, IMG_WIDTH))\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Data Generators\n",
        "# ---------------------------\n",
        "\n",
        "# Data augmentation settings\n",
        "AUG_PARAMS = dict(\n",
        "    rescale=1./255,\n",
        "    rotation_range=170,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "train_datagen_orig = ImageDataGenerator(**AUG_PARAMS)\n",
        "train_datagen_masked = ImageDataGenerator(**AUG_PARAMS) # Same augmentations for masked data\n",
        "\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255) # Only scaling for validation/test\n",
        "\n",
        "# 4a. Original Images Generator\n",
        "train_generator_orig = train_datagen_orig.flow_from_directory(\n",
        "    DATA_DIRS[\"train_orig\"],\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE_HALF,\n",
        "    class_mode='categorical',\n",
        "    color_mode='rgb',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# 4b. Pre-processed Masked Images Generator (reads from fast /tmp folder)\n",
        "train_generator_masked = train_datagen_masked.flow_from_directory(\n",
        "    MASKED_TRAIN_DIR,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE_HALF,\n",
        "    class_mode='categorical',\n",
        "    color_mode='rgb',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# 4c. Validation and Test Generators (read from Drive, no masking)\n",
        "validation_generator = val_test_datagen.flow_from_directory(\n",
        "    DATA_DIRS[\"validation\"],\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    color_mode='rgb',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    DATA_DIRS[\"test\"],\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    color_mode='rgb',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "class_names = list(train_generator_orig.class_indices.keys())\n",
        "print(\"Detected class names:\", class_names)\n",
        "print(f\"Train samples (Original): {train_generator_orig.samples}, Train samples (Masked): {train_generator_masked.samples}\")\n",
        "print(f\"Val: {validation_generator.samples}, Test: {test_generator.samples}\")\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Optimized Combined generator (No OpenCV calls)\n",
        "# ---------------------------\n",
        "class CombinedGeneratorOptimized(tf.keras.utils.Sequence):\n",
        "    \"\"\"Combines two generators where both are already processed/augmented.\"\"\"\n",
        "    def __init__(self, generator_orig, generator_masked):\n",
        "        self.generator1 = generator_orig\n",
        "        self.generator2 = generator_masked\n",
        "        assert generator_orig.batch_size == generator_masked.batch_size, \"Batch sizes must match.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.generator1), len(self.generator2))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Fetch two augmented half-batches\n",
        "        X1, y1 = self.generator1[index]\n",
        "        X2, y2 = self.generator2[index]\n",
        "\n",
        "        # Concatenate and shuffle\n",
        "        X = np.concatenate([X1, X2], axis=0)\n",
        "        y = np.concatenate([y1, y2], axis=0)\n",
        "\n",
        "        idx = np.arange(X.shape[0])\n",
        "        np.random.shuffle(idx)\n",
        "\n",
        "        return X[idx], y[idx]\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.generator1.on_epoch_end()\n",
        "        self.generator2.on_epoch_end()\n",
        "\n",
        "combined_train_generator = CombinedGeneratorOptimized(train_generator_orig, train_generator_masked)\n",
        "print(\"Combined generator batch size (per call):\", BATCH_SIZE)\n",
        "\n",
        "# ---------------------------\n",
        "# 6. Build and Train Model\n",
        "# ---------------------------\n",
        "\n",
        "# Build MobileNetV2 Base Model\n",
        "base_model = MobileNetV2(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), include_top=False, weights='imagenet')\n",
        "base_model.trainable = False\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Callbacks\n",
        "model_checkpoint_path = os.path.join(base_google_drive_path, \"best_model.keras\")\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1),\n",
        "    ModelCheckpoint(model_checkpoint_path, save_best_only=True, monitor=\"val_loss\")\n",
        "]\n",
        "\n",
        "# Phase 1: Feature Extraction\n",
        "print(\"\\n--- Starting Phase 1: Feature Extraction ---\")\n",
        "model.compile(optimizer=Adam(learning_rate=3e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history1 = model.fit(\n",
        "    combined_train_generator,\n",
        "    epochs=EPOCHS_PHASE1,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=callbacks,\n",
        "    steps_per_epoch=len(combined_train_generator)\n",
        ")\n",
        "\n",
        "# Phase 2: Fine-Tuning\n",
        "print(\"\\n--- Starting Phase 2: Fine-Tuning ---\")\n",
        "base_model.trainable = True\n",
        "\n",
        "# Unfreeze last 25% of the base model layers\n",
        "fine_tune_at = int(len(base_model.layers) * 0.75)\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history2 = model.fit(\n",
        "    combined_train_generator,\n",
        "    epochs=EPOCHS_PHASE1 + EPOCHS_PHASE2,\n",
        "    initial_epoch=EPOCHS_PHASE1,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=callbacks,\n",
        "    steps_per_epoch=len(combined_train_generator)\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# 7. Evaluate and Export\n",
        "# ---------------------------\n",
        "\n",
        "# Plot training curves\n",
        "acc = history1.history.get('accuracy', []) + history2.history.get('accuracy', [])\n",
        "val_acc = history1.history.get('val_accuracy', []) + history2.history.get('val_accuracy', [])\n",
        "loss = history1.history.get('loss', []) + history2.history.get('loss', [])\n",
        "val_loss = history1.history.get('val_loss', []) + history2.history.get('val_loss', [])\n",
        "\n",
        "min_epochs = min(len(acc), len(val_acc), len(loss), len(val_loss))\n",
        "acc = acc[:min_epochs]\n",
        "val_acc = val_acc[:min_epochs]\n",
        "loss = loss[:min_epochs]\n",
        "val_loss = val_loss[:min_epochs]\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Train Accuracy')\n",
        "plt.plot(val_acc, label='Val Accuracy')\n",
        "plt.axvline(x=EPOCHS_PHASE1 - 1, color='g', linestyle='--', label='Start Fine-Tune')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Train Loss')\n",
        "plt.plot(val_loss, label='Val Loss')\n",
        "plt.axvline(x=EPOCHS_PHASE1 - 1, color='g', linestyle='--')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Load the best saved model for final evaluation\n",
        "model = tf.keras.models.load_model(model_checkpoint_path)\n",
        "test_loss, test_acc = model.evaluate(test_generator, verbose=0)\n",
        "print(f\"Test loss (Best Model): {test_loss:.4f}, Test accuracy (Best Model): {test_acc:.4f}\")\n",
        "\n",
        "# Predictions and Confusion Matrix\n",
        "test_generator.reset()\n",
        "y_pred_probs = model.predict(test_generator, verbose=0)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "y_true = test_generator.classes\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d',\n",
        "             xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
        "\n",
        "# TFLite Conversion\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "def representative_data_gen():\n",
        "    \"\"\"Generator using pre-processed data for TFLite quantization calibration.\"\"\"\n",
        "    for i in range(min(50, len(combined_train_generator))):\n",
        "        X, _ = combined_train_generator[i]\n",
        "        yield [X.astype(np.float32)]\n",
        "\n",
        "converter.representative_dataset = representative_data_gen\n",
        "tflite_model = converter.convert()\n",
        "tflite_save_path = os.path.join(base_google_drive_path, 'plant_identification_model_int8.tflite')\n",
        "with open(tflite_save_path, 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "print(\"Saved TFLite model to:\", tflite_save_path)\n",
        "\n",
        "# Cleanup the temporary directory\n",
        "shutil.rmtree(MASKED_TRAIN_DIR)\n",
        "print(f\"Cleaned up temporary masked data directory: {MASKED_TRAIN_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "h7T3wSavW0cP",
        "outputId": "c61b8a84-0fd7-4a2b-8a6a-dee6d13b952b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "--- Starting One-Time Preprocessing (Saving to /tmp for speed) ---\n",
            "--- Preprocessing Complete. 130 images saved to /tmp/masked_training_data/ ---\n",
            "Found 130 images belonging to 4 classes.\n",
            "Found 130 images belonging to 4 classes.\n",
            "Found 19 images belonging to 4 classes.\n",
            "Found 38 images belonging to 4 classes.\n",
            "Detected class names: ['Drypetes Variabilis', 'Hieronyma oblonga', 'Manihot Brachyloba ', 'Manihot Esculenta ']\n",
            "Train samples (Original): 130, Train samples (Masked): 130\n",
            "Val: 19, Test: 38\n",
            "Combined generator batch size (per call): 32\n",
            "\n",
            "--- Starting Phase 1: Feature Extraction ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - accuracy: 0.3826 - loss: 1.5045 - val_accuracy: 0.3158 - val_loss: 1.9851 - learning_rate: 3.0000e-04\n",
            "Epoch 2/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2s/step - accuracy: 0.4527 - loss: 1.4079 - val_accuracy: 0.3158 - val_loss: 1.6169 - learning_rate: 3.0000e-04\n",
            "Epoch 3/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.5607 - loss: 1.1203 - val_accuracy: 0.2632 - val_loss: 1.6512 - learning_rate: 3.0000e-04\n",
            "Epoch 4/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - accuracy: 0.5774 - loss: 1.0531 - val_accuracy: 0.2632 - val_loss: 1.5486 - learning_rate: 3.0000e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.5415 - loss: 1.0496 - val_accuracy: 0.2632 - val_loss: 1.5935 - learning_rate: 3.0000e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.5801 - loss: 0.9590 - val_accuracy: 0.2632 - val_loss: 1.5301 - learning_rate: 3.0000e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.6244 - loss: 0.9224 - val_accuracy: 0.2632 - val_loss: 1.5247 - learning_rate: 3.0000e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.6688 - loss: 0.8154 - val_accuracy: 0.2632 - val_loss: 1.5584 - learning_rate: 3.0000e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - accuracy: 0.6454 - loss: 0.8181 - val_accuracy: 0.2632 - val_loss: 1.5949 - learning_rate: 3.0000e-04\n",
            "Epoch 10/10\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0j6RB1eRqi75qNCA/oyiv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}